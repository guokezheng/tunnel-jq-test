#------开发环境配置-----
spring:
  # 数据源配置
  datasource:
    druid:
      # 主库数据源
      master:
        url: jdbc:mysql://10.168.56.204:3306/tunnel-platform?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=false&serverTimezone=GMT%2B8&allowMultiQueries=true
        username: root
        password: Platform123!@#
#测试kafka配置
  kafka:
    wulian:
      bootstrap-servers: 10.168.56.206:9092
      producer:
        # 重试次数，设置大于0的值，则客户端会将发送失败的记录重新发送
        #        retries: 3
        #        batch-size: 16384 #批量处理大小，16K
        #        buffer-memory: 33554432 #缓冲存储大，32M
        #        acks: 1
        # 指定消息key和消息体的编解码方式
        key-serializer: org.apache.kafka.common.serialization.StringSerializer
        value-serializer: org.apache.kafka.common.serialization.StringSerializer
        security:
          protocol: SASL_PLAINTEXT
          properties:
            sasl:
              mechanism: SCRAM-SHA-256
              jaas:
                config: org.apache.kafka.common.security.scram.ScramLoginModule required username='admin' password='admin@123';
      consumer:
        group-id: test
        enable-auto-commit: false # 不自动签收
        auto-offset-reset: earliest  #kafka出现错误重启后，会找到未消费的offset继续消费
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        max-poll-records: 5
        security:
          protocol: SASL_PLAINTEXT
          properties:
            sasl:
              mechanism: SCRAM-SHA-256
              jaas:
                config: org.apache.kafka.common.security.scram.ScramLoginModule required username='admin' password='admin@123';

    wanji:
      #非正式环境使用地址，避免接收推送数据
      bootstrap-servers: 10.7.179.15:9092
      # 生产者
      producer:
        # 重试次数，设置大于0的值，则客户端会将发送失败的记录重新发送
        retries: 3
        batch-size: 16384 #批量处理大小，16K
        buffer-memory: 33554432 #缓冲存储大，32M
        acks: 1
        # 指定消息key和消息体的编解码方式
        key-serializer: org.apache.kafka.common.serialization.StringSerializer
        value-serializer: org.apache.kafka.common.serialization.StringSerializer

      # 消费者
      consumer:
        # 消费者组
        group-id: HuShanGroup_Formal
        # 小车前端显示消费者组
        group-id-Two: HuShanGroup_Formal_Cat
        # 是否自动提交
        enable-auto-commit: false
        # 消费偏移配置
        # none：如果没有为消费者找到先前的offset的值,即没有自动维护偏移量,也没有手动维护偏移量,则抛出异常
        # earliest：在各分区下有提交的offset时：从offset处开始消费；在各分区下无提交的offset时：从头开始消费
        # latest：在各分区下有提交的offset时：从offset处开始消费；在各分区下无提交的offset时：从最新的数据开始消费
        auto-offset-reset: earliest
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        max-poll-records: 500
      properties:
        max.poll.interval.ms: 1000000

